{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe594ae2",
   "metadata": {},
   "source": [
    "Class for custom Imbalanced MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cfb07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as tvt\n",
    "from PIL import Image\n",
    "\n",
    "class BinaryImbalancedMNIST(Dataset):\n",
    "\n",
    "  def __init__(self,proportion,val_prop=0.5,num_val_images = 10, mode=\"train\"):\n",
    "    super(BinaryImbalancedMNIST, self).__init__()\n",
    "    num_images = 5000\n",
    "    classes = [9,4]\n",
    "    num_val_images = num_val_images\n",
    "    \n",
    "    self.images = []\n",
    "    self.labels = []\n",
    "    self.val_images = []\n",
    "    self.val_labels = []\n",
    "\n",
    "    if(mode == \"train\"):\n",
    "      self.mnist = datasets.MNIST('data',train=True,download=True,transform=None)\n",
    "    else:\n",
    "      self.mnist = datasets.MNIST('data',train=False,download=True,transform=None)\n",
    "      num_val_images=0\n",
    "    \n",
    "    #standard recommended transform for MNIST by Pytorch\n",
    "    self.transform = tvt.Compose([tvt.Resize((32,32)),\n",
    "                                  tvt.ToTensor(),\n",
    "                                  tvt.Normalize((0.1307,),(0.3081,))\n",
    "                                  ])\n",
    "    num_images_class0 = int(np.floor(num_images*proportion))\n",
    "    num_images_class1 = num_images - num_images_class0\n",
    "    num_images_per_class = [num_images_class0,num_images_class1]\n",
    "    \n",
    "    num_val_images_class0 = int(np.floor(num_val_images*val_prop))\n",
    "    num_val_images_class1 = num_val_images - num_val_images_class0\n",
    "    num_val_images_per_class = [num_val_images_class0,num_val_images_class1]\n",
    "    \n",
    "    if mode == \"train\":\n",
    "      data = self.mnist.train_data\n",
    "      label = self.mnist.train_labels\n",
    "    else:\n",
    "      data = self.mnist.test_data\n",
    "      label = self.mnist.test_labels\n",
    "\n",
    "    for idx, cls in enumerate(classes):\n",
    "      # find idx where label is 9 or 4\n",
    "      data_idx = np.where(label == cls)[0]\n",
    "      # get images for each class based on idx found. Keep num_val_images for validation dataset\n",
    "      imgs = data[data_idx[0:num_images_per_class[idx] - num_val_images_per_class[idx]]]\n",
    "      self.images.extend(imgs)\n",
    "      # get labels for each class based on idx found. \n",
    "      cls_idx = label[data_idx[0:num_images_per_class[idx] - num_val_images_per_class[idx]]]\n",
    "      # set label as 1 for class = 9 and 0 for class = 4\n",
    "      self.labels.extend((cls_idx == classes[0]).float())\n",
    "\n",
    "      #if training dataset, also create the validation data. Skip for testing\n",
    "      if mode == \"train\":\n",
    "        val_imgs = data[data_idx[num_images_per_class[idx]-num_val_images_per_class[idx]:num_images_per_class[idx]]]\n",
    "        for img in val_imgs:\n",
    "          img_tmp = Image.fromarray(img.numpy(), mode='L')\n",
    "          img_tmp = self.transform(img_tmp)\n",
    "          self.val_images.append(img_tmp.unsqueeze(0))\n",
    "        val_cls = label[data_idx[num_images_per_class[idx]-num_val_images_per_class[idx]:num_images_per_class[idx]]]\n",
    "        self.val_labels.append((val_cls == classes[0]).float())\n",
    "    if mode == \"train\":\n",
    "      self.val_images = torch.cat(self.val_images, dim=0)\n",
    "      self.val_labels = torch.cat(self.val_labels, dim=0)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    image, label = self.images[idx],self.labels[idx]\n",
    "    image = Image.fromarray(image.numpy(), mode='L')\n",
    "    if self.transform is not None:\n",
    "       image = self.transform(image).to(dtype=torch.float64)\n",
    "    label = torch.tensor(label)\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e83020",
   "metadata": {},
   "source": [
    "Example - imbalanced training data vs balanced validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "train_data = BinaryImbalancedMNIST(proportion=0.95, mode=\"train\")\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=10,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933fe945",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image,label) in enumerate(train_data_loader):\n",
    "    print(image.size())\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a6891",
   "metadata": {},
   "source": [
    "Validation Data - Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993087f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BinaryImbalancedMNIST(proportion=0.95, mode=\"train\")\n",
    "val_data = train_data.val_images\n",
    "val_label = train_data.val_labels\n",
    "print(val_data.size())\n",
    "print(val_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de3aca",
   "metadata": {},
   "source": [
    "MetaModule class to enable Meta Learnings. Reference - https://github.com/danieltan07/learning-to-reweight-examples/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb633e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "\n",
    "# much of the MetaModule is adopted from the work done by Adrien Ecoffet. This\n",
    "# MetaModule is needed for - https://discuss.pytorch.org/t/higher-order-derivatives-meta-learning/93051/8, https://discuss.pytorch.org/t/second-order-derivatives-in-meta-learning/76656/2\n",
    "# https://discuss.pytorch.org/t/higher-order-derivatives-meta-learning/93051, https://discuss.pytorch.org/t/cannot-calculate-second-order-gradients-even-though-create-graph-true/78711/3\n",
    "\n",
    "def to_var(x, requires_grad=True):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, requires_grad=requires_grad)\n",
    "\n",
    "class MetaModule(nn.Module):\n",
    "    # adopted from: Adrien Ecoffet https://github.com/AdrienLE\n",
    "    def params(self):\n",
    "       for name, param in self.named_params(self):\n",
    "            yield param\n",
    "    \n",
    "    def named_leaves(self):\n",
    "        return []\n",
    "    \n",
    "    def named_submodules(self):\n",
    "        return []\n",
    "    \n",
    "    def named_params(self, curr_module=None, memo=None, prefix=''):       \n",
    "        if memo is None:\n",
    "            memo = set()\n",
    "\n",
    "        if hasattr(curr_module, 'named_leaves'):\n",
    "            for name, p in curr_module.named_leaves():\n",
    "                if p is not None and p not in memo:\n",
    "                    memo.add(p)\n",
    "                    yield prefix + ('.' if prefix else '') + name, p\n",
    "        else:\n",
    "            for name, p in curr_module._parameters.items():\n",
    "                if p is not None and p not in memo:\n",
    "                    memo.add(p)\n",
    "                    yield prefix + ('.' if prefix else '') + name, p\n",
    "                    \n",
    "        for mname, module in curr_module.named_children():\n",
    "            submodule_prefix = prefix + ('.' if prefix else '') + mname\n",
    "            for name, p in self.named_params(module, memo, submodule_prefix):\n",
    "                yield name, p\n",
    "    \n",
    "    def update_params(self, lr_inner, first_order=False, source_params=None, detach=False):\n",
    "        if source_params is not None:\n",
    "            for tgt, src in zip(self.named_params(self), source_params):\n",
    "                name_t, param_t = tgt\n",
    "                # name_s, param_s = src\n",
    "                # grad = param_s.grad\n",
    "                # name_s, param_s = src\n",
    "                grad = src\n",
    "                if first_order:\n",
    "                    grad = to_var(grad.detach().data)\n",
    "                tmp = param_t - lr_inner * grad\n",
    "                self.set_param(self, name_t, tmp)\n",
    "        else:\n",
    "\n",
    "            for name, param in self.named_params(self):\n",
    "                if not detach:\n",
    "                    grad = param.grad\n",
    "                    if first_order:\n",
    "                        grad = to_var(grad.detach().data)\n",
    "                    tmp = param - lr_inner * grad\n",
    "                    self.set_param(self, name, tmp)\n",
    "                else:\n",
    "                    param = param.detach_()\n",
    "                    self.set_param(self, name, param)\n",
    "\n",
    "    def set_param(self,curr_mod, name, param):\n",
    "        if '.' in name:\n",
    "            n = name.split('.')\n",
    "            module_name = n[0]\n",
    "            rest = '.'.join(n[1:])\n",
    "            for name, mod in curr_mod.named_children():\n",
    "                if module_name == name:\n",
    "                    self.set_param(mod, rest, param)\n",
    "                    break\n",
    "        else:\n",
    "            setattr(curr_mod, name, param)\n",
    "            \n",
    "    def detach_params(self):\n",
    "        for name, param in self.named_params(self):\n",
    "            self.set_param(self, name, param.detach())   \n",
    "                \n",
    "    def copy(self, other, same_var=False):\n",
    "        for name, param in other.named_params():\n",
    "            if not same_var:\n",
    "                param = to_var(param.data.clone(), requires_grad=True)\n",
    "            self.set_param(name, param)\n",
    "\n",
    "\n",
    "class MetaLinear(MetaModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        ignore = nn.Linear(*args, **kwargs)\n",
    "       \n",
    "        self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n",
    "        self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "    \n",
    "    def named_leaves(self):\n",
    "        return [('weight', self.weight), ('bias', self.bias)]\n",
    "    \n",
    "class MetaConv2d(MetaModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        ignore = nn.Conv2d(*args, **kwargs)\n",
    "        \n",
    "        self.stride = ignore.stride\n",
    "        self.padding = ignore.padding\n",
    "        self.dilation = ignore.dilation\n",
    "        self.groups = ignore.groups\n",
    "        \n",
    "        self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n",
    "        \n",
    "        if ignore.bias is not None:\n",
    "            self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "    \n",
    "    def named_leaves(self):\n",
    "        return [('weight', self.weight), ('bias', self.bias)]\n",
    "\n",
    "class MetaLeNet5(MetaModule):\n",
    "    def __init__(self, n_out):\n",
    "        super(MetaLeNet5, self).__init__()\n",
    "    \n",
    "        layers = []\n",
    "        layers.append(MetaConv2d(1, 6, kernel_size=5))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "\n",
    "        layers.append(MetaConv2d(6, 16, kernel_size=5))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "        \n",
    "        layers.append(MetaConv2d(16, 120, kernel_size=5))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.cnn = nn.Sequential(*layers)\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(MetaLinear(120, 84))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(MetaLinear(84, n_out))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(-1, 120)\n",
    "        return self.fc_layers(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da150e",
   "metadata": {},
   "source": [
    "Training Loop - baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_training_baseline(net,device):\n",
    "  net = copy.deepcopy(net)\n",
    "  net = net.to(device)\n",
    "  optimizer = torch.optim.SGD(net.params(),lr=hyperparameters['lr'])\n",
    "  print(\"\\n\\nStarting training loop...\")\n",
    "  start_time = time.perf_counter()\n",
    "  elapsed_time = 0.0\n",
    "  plot_step = 100\n",
    "  print_step = 1000\n",
    "  batch = hyperparameters['batch_size']\n",
    "  running_loss = []\n",
    "  test_running_loss = []\n",
    "  test_accuracy = []\n",
    "  test_error = []\n",
    "  iteration_count = []\n",
    "  train_running_loss = 0.0\n",
    "  for i in tqdm(range(hyperparameters['num_iterations'])):\n",
    "    net.train()\n",
    "    images, labels = next(iter(train_data_loader))\n",
    "    if(i%print_step) == (print_step - 1):\n",
    "      current_time = time.perf_counter()\n",
    "      elapsed_time = current_time - start_time \n",
    "      print(\"\\n\\niter=%4d: elapsed_time=%5d secs] Ground Truth:     \" % ((i+1), elapsed_time) + \n",
    "                        ' '.join('%5s' % [labels[j].item()] for j in range(batch)))\n",
    "    images = images.to(device).float()\n",
    "    labels = labels.to(device)\n",
    "    outputs = net(images)\n",
    "    loss = F.binary_cross_entropy_with_logits(outputs,labels)\n",
    "    if(i%print_step) == (print_step - 1):\n",
    "      predicted = (F.sigmoid(outputs)>0.5).int()\n",
    "      print(\"\\n\\n[iter=%4d: elapsed_time=%5d secs] Predicted Labels:     \" % ((i+1), elapsed_time) + \n",
    "                         ' '.join('%5s' % [predicted[j].item()] for j in range(batch)))\n",
    "      print(\"\\n[iter:%4d] loss:%.2f\"%(i+1, avg_loss))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_running_loss += loss.item()\n",
    "    if(i%plot_step) == (plot_step-1):\n",
    "      avg_loss = train_running_loss/float(plot_step)\n",
    "      running_loss.append(avg_loss)\n",
    "      iteration_count.append(i)\n",
    "      train_running_loss = 0.0\n",
    "\n",
    "      net.eval()\n",
    "      with torch.no_grad():\n",
    "        testing_loss = 0.0\n",
    "        totalRight = 0\n",
    "        error = 0\n",
    "        total = 0\n",
    "        acc = []\n",
    "        for j, data in enumerate(test_data_loader):\n",
    "          images, label = data\n",
    "          images = images.to(device).float()\n",
    "          label = label.to(device)\n",
    "          outputs = net(images)\n",
    "          predicted = (F.sigmoid(outputs)>0.5).float()\n",
    "          loss = F.binary_cross_entropy_with_logits(outputs,label)\n",
    "          testing_loss += loss\n",
    "          totalRight += (predicted == label).sum().item()\n",
    "          total += label.size(0)\n",
    "        test_running_loss.append(testing_loss/float(j+1))\n",
    "        test_accuracy.append(100 * totalRight / float(total))\n",
    "        test_error.append(100*(total-totalRight)/float(total))\n",
    "  return running_loss, test_error, test_accuracy, iteration_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0852a",
   "metadata": {},
   "source": [
    "__MAIN__ for running baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c812998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "hyperparameters = {\n",
    "        'lr' : 1e-3,\n",
    "        'momentum' : 0.9,\n",
    "        'batch_size' : 100,\n",
    "        'num_iterations' : 8000\n",
    "}\n",
    "\n",
    "train_data = BinaryImbalancedMNIST(proportion=0.995, mode=\"train\")\n",
    "test_data = BinaryImbalancedMNIST(proportion=0.5, mode=\"test\")\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=hyperparameters['batch_size'],shuffle=True,num_workers=0)\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,batch_size=hyperparameters['batch_size'],shuffle=True,num_workers=0)\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"device: \",device)\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmarks = False\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "num_classes=100\n",
    "\n",
    "model = MetaLeNet5(n_out=1)\n",
    "\n",
    "running_loss, test_error, test_accuracy, iteration_count = run_training_baseline(model,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc49e123",
   "metadata": {},
   "source": [
    "Plot baseline results - Accuracy vs Iteration & Error vs Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb845db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13,5))\n",
    "ax1, ax2 = axes.ravel()\n",
    "ax1.plot(iteration_count,running_loss, label='Training Loss')\n",
    "ax1.set_ylabel(\"Losses\")\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(iteration_count, test_accuracy, label=\"Test Accuracy\")\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.legend()\n",
    "plt.savefig(\"loss_accuracy_0_995.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(iteration_count, test_error, label=\"Test error\")\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.savefig(\"test_error_0_995.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Final test accuracy: \", test_accuracy[-1],\" test error: \", test_error[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3e8c0",
   "metadata": {},
   "source": [
    "LRE performance - MetaLeNet5 on imbalanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd01700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_training_w_lre(net,device):\n",
    "  net = copy.deepcopy(net)\n",
    "  net = net.to(device)\n",
    "  optimizer = torch.optim.SGD(net.params(),lr=hyperparameters['lr'])\n",
    "  print(\"\\n\\nStarting training loop...\")\n",
    "  start_time = time.perf_counter()\n",
    "  elapsed_time = 0.0\n",
    "  plot_step = 100\n",
    "  print_step = 1000\n",
    "  batch = hyperparameters['batch_size']\n",
    "  running_loss = []\n",
    "  test_running_loss = []\n",
    "  test_accuracy = []\n",
    "  test_error = []\n",
    "  iteration_count = []\n",
    "  train_running_loss = 0.0\n",
    "  for i in tqdm(range(hyperparameters['num_iterations'])):\n",
    "    net.train()\n",
    "    x_f, y_f = next(iter(train_data_loader))\n",
    "    if(i%print_step) == (print_step - 1):\n",
    "      current_time = time.perf_counter()\n",
    "      elapsed_time = current_time - start_time \n",
    "      print(\"\\n\\niter=%4d: elapsed_time=%5d secs] Ground Truth:     \" % ((i+1), elapsed_time) + \n",
    "                        ' '.join('%5s' % [y_f[j].item()] for j in range(batch)))\n",
    "    x_f = x_f.to(device).float()\n",
    "    y_f = y_f.to(device)\n",
    "\n",
    "\n",
    "    # Since this method requires 3 passes of the network, we create a dummy network\n",
    "    # that is used for the inital 2 passes which gives us the gradient with respect\n",
    "    # to model parameters and example weights. The last pass with the optimal weights\n",
    "    # will be performed on the actual model we train.\n",
    "    dummy_net = MetaLeNet5(n_out=1)\n",
    "    dummy_net.load_state_dict(net.state_dict())\n",
    "    dummy_net = dummy_net.to(device)\n",
    "\n",
    "    # 1st Pass: training data pass to compute initial weighted loss\n",
    "    y_f_pred = dummy_net(x_f)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_f_pred,y_f,reduce=False)\n",
    "    # intialize epsilon with 0s as initial weight\n",
    "    eps = (torch.zeros(loss.size())).to(device)\n",
    "    eps.requires_grad = True\n",
    "    l_f = torch.sum(loss*eps)\n",
    "\n",
    "    dummy_net.zero_grad()\n",
    "\n",
    "    #perform parameter update using the MetaModule class\n",
    "    gradient = torch.autograd.grad(l_f,(dummy_net.params()),create_graph=True)\n",
    "    dummy_net.update_params(hyperparameters['lr'],source_params=gradient)\n",
    "\n",
    "    # 2nd Pass: val data pass to compute gradient with the weights\n",
    "    y_g_pred = dummy_net(val_images)\n",
    "    l_g = F.binary_cross_entropy_with_logits(y_g_pred,val_labels)\n",
    "    eps_gradient = torch.autograd.grad(l_g,eps,only_inputs=True)[0]\n",
    "\n",
    "    # Normalize the gradient and ceil to avoid negative weights\n",
    "    w_hat = torch.clamp(-eps_gradient,min=0)\n",
    "    sum = torch.sum(w_hat)\n",
    "\n",
    "    if (sum!=0):\n",
    "      weights = w_hat/sum\n",
    "    else:\n",
    "      weights = w_hat\n",
    "\n",
    "    # 3rd Pass: training data pass through original network to compute weighted\n",
    "    #           loss and perform parameter update\n",
    "\n",
    "    y_f_pred_1 = net(x_f)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_f_pred_1,y_f,reduce=False)\n",
    "    l_f = torch.sum(loss*weights)\n",
    "\n",
    "    if(i%print_step) == (print_step - 1):\n",
    "      predicted = (F.sigmoid(y_f_pred)>0.5).int()\n",
    "      print(\"\\n\\n[iter=%4d: elapsed_time=%5d secs] Predicted Labels:     \" % ((i+1), elapsed_time) + \n",
    "                         ' '.join('%5s' % [predicted[j].item()] for j in range(batch)))\n",
    "      print(\"\\n[iter:%4d] loss:%.2f\"%(i+1, avg_loss))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    l_f.backward()\n",
    "    optimizer.step()\n",
    "    train_running_loss += l_f.item()\n",
    "    if(i%plot_step) == (plot_step-1):\n",
    "      avg_loss = train_running_loss/float(plot_step)\n",
    "      running_loss.append(avg_loss)\n",
    "      iteration_count.append(i)\n",
    "      train_running_loss = 0.0\n",
    "\n",
    "      net.eval()\n",
    "      with torch.no_grad():\n",
    "        testing_loss = 0.0\n",
    "        totalRight = 0\n",
    "        error = 0\n",
    "        total = 0\n",
    "        for j, data in enumerate(test_data_loader):\n",
    "          images, label = data\n",
    "          images = images.to(device).float()\n",
    "          label = label.to(device)\n",
    "          outputs = net(images)\n",
    "          predicted = (F.sigmoid(outputs)>0.5).float()\n",
    "          loss = F.binary_cross_entropy_with_logits(outputs,label)\n",
    "          testing_loss += loss\n",
    "          totalRight += (predicted == label).sum().item()\n",
    "          total += label.size(0)\n",
    "        test_running_loss.append(testing_loss/float(j+1))\n",
    "        test_accuracy.append(100 * totalRight / float(total))\n",
    "        test_error.append(100*(total-totalRight)/float(total))\n",
    "  return running_loss, test_error, test_accuracy, iteration_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1aa64b",
   "metadata": {},
   "source": [
    "__MAIN__ for LRE with varying proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "hyperparameters = {\n",
    "        'lr' : 1e-3,\n",
    "        'momentum' : 0.9,\n",
    "        'batch_size' : 100,\n",
    "        'num_iterations' : 8000\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"device: \",device)\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmarks = False\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "num_classes=100\n",
    "\n",
    "test_data = BinaryImbalancedMNIST(proportion=0.5, mode=\"test\")\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,batch_size=hyperparameters['batch_size'],shuffle=True,num_workers=0)\n",
    "\n",
    "proportions = [0.9, 0.95, 0.98, 0.99, 0.995]\n",
    "accuracy_accumulated = {}\n",
    "loss_accumulated = {}\n",
    "error_accumulated = {}\n",
    "num_trials = 5\n",
    "\n",
    "for prop in proportions:\n",
    "  train_data = BinaryImbalancedMNIST(proportion=prop, mode=\"train\")\n",
    "  train_data_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=hyperparameters['batch_size'],shuffle=True,num_workers=0)\n",
    "  val_images = (train_data_loader.dataset.val_images).to(device)\n",
    "  val_labels = (train_data_loader.dataset.val_labels).to(device)\n",
    "\n",
    "  for idx in range(num_trials):\n",
    "    model = MetaLeNet5(n_out=1)\n",
    "    running_loss, test_error, test_accuracy, iteration_count = run_training_w_lre(model,device)\n",
    "    \n",
    "    if(prop in accuracy_accumulated):\n",
    "      accuracy_accumulated[prop].append(test_accuracy)\n",
    "      loss_accumulated[prop].append(running_loss)\n",
    "      error_accumulated[prop].append(test_error)\n",
    "    else:\n",
    "      accuracy_accumulated[prop] = [test_accuracy]\n",
    "      loss_accumulated[prop] = [running_loss]\n",
    "      error_accumulated[prop] = [test_error]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c135e",
   "metadata": {},
   "source": [
    "Plot results for varying proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190034ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_loss = {}\n",
    "mean_accuracy = {}\n",
    "mean_error = {}\n",
    "\n",
    "for prop in proportions:\n",
    "  mean_loss[prop] = np.mean([loss_accumulated[prop][0],loss_accumulated[prop][1],loss_accumulated[prop][2],loss_accumulated[prop][3],loss_accumulated[prop][4]],axis=0)\n",
    "  mean_accuracy[prop] = np.mean([accuracy_accumulated[prop][0],accuracy_accumulated[prop][1],accuracy_accumulated[prop][2],accuracy_accumulated[prop][3],accuracy_accumulated[prop][4]],axis=0)\n",
    "  mean_error[prop] = np.mean([error_accumulated[prop][0],error_accumulated[prop][1],error_accumulated[prop][2],error_accumulated[prop][3],error_accumulated[prop][4]],axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13,5))\n",
    "ax1, ax2 = axes.ravel()\n",
    "ax1.plot(iteration_count,mean_loss[0.9], label='0.9')\n",
    "ax1.plot(iteration_count,mean_loss[0.95], label='0.95')\n",
    "ax1.plot(iteration_count,mean_loss[0.98], label='0.98')\n",
    "ax1.plot(iteration_count,mean_loss[0.99], label='0.99')\n",
    "ax1.plot(iteration_count,mean_loss[0.995], label='0.995')\n",
    "ax1.set_ylabel(\"Training Losses\")\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(iteration_count, mean_accuracy[0.9], label=\"0.9\")\n",
    "ax2.plot(iteration_count, mean_accuracy[0.95], label=\"0.95\")\n",
    "ax2.plot(iteration_count, mean_accuracy[0.98], label=\"0.98\")\n",
    "ax2.plot(iteration_count, mean_accuracy[0.99], label=\"0.99\")\n",
    "ax2.plot(iteration_count, mean_accuracy[0.995], label=\"0.95\")\n",
    "ax2.set_ylabel('Test Accuracy')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.legend()\n",
    "plt.savefig(\"loss_accuracy.png\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = [mean_error[0.9][-1],\n",
    "         mean_error[0.95][-1],\n",
    "         mean_error[0.98][-1],\n",
    "         mean_error[0.99][-1],\n",
    "         mean_error[0.995][-1]]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(proportions, error, linestyle=\"--\", marker=\"*\", label=\"LRE weighted\")\n",
    "plt.ylabel('Error %')\n",
    "plt.xlabel('Proportions')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"error_w_proportions.png\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d041c4",
   "metadata": {},
   "source": [
    "LRE with noisy validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2962c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "hyperparameters = {\n",
    "        'lr' : 1e-3,\n",
    "        'momentum' : 0.9,\n",
    "        'batch_size' : 100,\n",
    "        'num_iterations' : 8000\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"device: \",device)\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmarks = False\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "num_classes=100\n",
    "\n",
    "test_data = BinaryImbalancedMNIST(proportion=0.5, mode=\"test\")\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,batch_size=hyperparameters['batch_size'],shuffle=True,num_workers=0)\n",
    "\n",
    "val_proportions = [0.5, 0.6, 0.8]\n",
    "accuracy_accumulated = {}\n",
    "loss_accumulated = {}\n",
    "error_accumulated = {}\n",
    "num_trials = 1\n",
    "\n",
    "for val_prop in val_proportions:\n",
    "  train_data = BinaryImbalancedMNIST(proportion=0.98, val_prop=val_prop, mode=\"train\")\n",
    "  train_data_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=hyperparameters['batch_size'],shuffle=True,num_workers=0)\n",
    "  val_images = (train_data_loader.dataset.val_images).to(device)\n",
    "  val_labels = (train_data_loader.dataset.val_labels).to(device)\n",
    "\n",
    "  for idx in range(num_trials):\n",
    "    model = MetaLeNet5(n_out=1)\n",
    "    running_loss, test_error, test_accuracy, iteration_count = run_training_w_lre(model,device)\n",
    "    \n",
    "    if(val_prop in accuracy_accumulated):\n",
    "      accuracy_accumulated[val_prop].append(test_accuracy)\n",
    "      loss_accumulated[val_prop].append(running_loss)\n",
    "      error_accumulated[val_prop].append(test_error)\n",
    "    else:\n",
    "      accuracy_accumulated[val_prop] = [test_accuracy]\n",
    "      loss_accumulated[val_prop] = [running_loss]\n",
    "      error_accumulated[val_prop] = [test_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(iteration_count,np.squeeze(accuracy_accumulated[0.5]), label='Unbiased validation set')\n",
    "plt.plot(iteration_count,np.squeeze(accuracy_accumulated[0.6]), label='Partly biased validation set')\n",
    "plt.plot(iteration_count,np.squeeze(accuracy_accumulated[0.8]), label='Biased validation set')\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend()\n",
    "plt.savefig(\"val_data_biased.png\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea34bbc",
   "metadata": {},
   "source": [
    "LRE with varying val_data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ba7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "hyperparameters = {\n",
    "        'lr' : 1e-3,\n",
    "        'momentum' : 0.9,\n",
    "        'batch_size' : 100,\n",
    "        'num_iterations' : 8000\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"device: \",device)\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmarks = False\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "num_classes=100\n",
    "\n",
    "test_data = BinaryImbalancedMNIST(proportion=0.5, mode=\"test\")\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,batch_size=hyperparameters['batch_size'],shuffle=True,num_workers=0)\n",
    "\n",
    "\n",
    "accuracy_accumulated = {}\n",
    "loss_accumulated = {}\n",
    "error_accumulated = {}\n",
    "num_trials = 1\n",
    "num_val_images = [5,10,20,30]\n",
    "\n",
    "for num_val_data in num_val_images:\n",
    "  train_data = BinaryImbalancedMNIST(proportion=0.98, val_prop=0.5, num_val_images = num_val_data, mode=\"train\")\n",
    "  train_data_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=hyperparameters['batch_size'],shuffle=True,num_workers=0)\n",
    "  val_images = (train_data_loader.dataset.val_images).to(device)\n",
    "  val_labels = (train_data_loader.dataset.val_labels).to(device)\n",
    "\n",
    "  for idx in range(num_trials):\n",
    "    model = MetaLeNet5(n_out=1)\n",
    "    running_loss, test_error, test_accuracy, iteration_count = run_training_w_lre(model,device)\n",
    "    \n",
    "    \n",
    "    accuracy_accumulated[num_val_data] = [test_accuracy]\n",
    "    loss_accumulated[num_val_data] = [running_loss]\n",
    "    error_accumulated[num_val_data] = [test_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133779c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(iteration_count,np.squeeze(accuracy_accumulated[5]), label='n_val = 5')\n",
    "plt.plot(iteration_count,np.squeeze(accuracy_accumulated[10]), label='n_val = 10')\n",
    "plt.plot(iteration_count,np.squeeze(accuracy_accumulated[20]), label='n_val = 20')\n",
    "plt.plot(iteration_count,np.squeeze(accuracy_accumulated[30]), label='n_val = 30')\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend()\n",
    "plt.savefig(\"n_val_variation.png\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8858ed",
   "metadata": {},
   "source": [
    "LRE comparison with other techniques. Numbers referenced from https://arxiv.org/abs/1803.09050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb76213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lre = [2,3,4,6.25,8.75]\n",
    "random = [2,4,6,10,18]\n",
    "proportion = [2,2.5,3.5,6,8]\n",
    "resample = [2,3,4,6,10]\n",
    "hardmine=[2,3,3.5,5,8]\n",
    "prop = [0.90,0.95,0.98,0.99,0.995]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(prop,lre, label='LRE')\n",
    "plt.plot(prop,random,linestyle='--',label='Random')\n",
    "plt.plot(prop,proportion,linestyle='-.',label='Proportion')\n",
    "plt.plot(prop,resample,linestyle=':',label='Resample')\n",
    "plt.plot(prop,hardmine,linestyle='--',label='Hard Mining')\n",
    "\n",
    "plt.ylabel(\"Test Error %\")\n",
    "plt.xlabel(\"Proportions\")\n",
    "plt.legend()\n",
    "plt.savefig(\"error_comparison\",bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac72e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "595ml",
   "language": "python",
   "name": "595ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
